{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookId":"652a8cdb-6c7e-46cc-ba54-70bb15cea257","language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.7.7","file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"code","source":"#!g1.1\nimport pandas as pd\ndf = pd.read_csv('cert_0.csv')","metadata":{"execution_id":"b58f83c9-690b-4887-9d2f-ba8bcbcea820","cellId":"qjdslt4xbecgg08vuatmp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndf2 = df.sort_values(by=['reasons'], ascending=False, axis=0)\ndf2 = df2.fillna(0)\ndf2['weight'] = df2['weight'].apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(0)\ndf2['base_weight'] = df2['base_weight'].apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(0)","metadata":{"cellId":"d381rptgwawa4wlfd4d6te","trusted":true},"outputs":[],"execution_count":663},{"cell_type":"code","source":"#!g1.1\ndf2.head()","metadata":{"cellId":"pm09t0bykchltfbbbn7w5h","trusted":true},"outputs":[],"execution_count":664},{"cell_type":"code","source":"#!g1.1\n df2 = df2.drop(['Unnamed: 0'],1)","metadata":{"cellId":"4e0zakunxxh2jjurlb6htw","trusted":true},"outputs":[],"execution_count":665},{"cell_type":"code","source":"#!g1.1\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nimport tensorflow.keras as k\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.python.keras.layers.core import Dense\nfrom tensorflow.python.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n\n# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n# physical_devices = tf.config.list_physical_devices('GPU')\n# try:\n#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n# except:\n#     pass","metadata":{"cellId":"1tqdzd0xnyi49b1qi4tub6","trusted":true},"outputs":[],"execution_count":661},{"cell_type":"code","source":"#!g1.1\ndf2 = df2.drop(['canceled_cert_date'], axis=1).astype({'consignee_be_sub_region_id': str, 'consignee_ent_sub_region_id': str, 'consignor_be_sub_region_id': str, 'consignor_ent_sub_region_id': str, 'guidRecipientCountry': str, 'repaid_cert_date': str})\ndataset = df_to_dataset(df2)","metadata":{"cellId":"4b77erm0ixymcevygporv","trusted":true},"outputs":[],"execution_count":666},{"cell_type":"code","source":"#!g1.1\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  dataframe = dataframe.copy()\n  labels = dataframe.pop('reasons')\n  print(labels)\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), to_categorical(labels, num_classes=9)))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  ds = ds.prefetch(batch_size)\n  return ds","metadata":{"cellId":"dad2qpz0drj1l5tn04g7q1","trusted":true},"outputs":[],"execution_count":667},{"cell_type":"code","source":"#!g1.1\nimport numpy as mp\nbatch_size = 5\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\n[(train_features, label_batch)] = train_ds.take(1)\nprint('Every feature:', list(train_features.keys()))\nprint('Status id:', train_features['cert_status_id'])\nprint('A batch of targets:', label_batch)","metadata":{"cellId":"kli9b8ubssjnmbm6zb21ys","trusted":true},"outputs":[],"execution_count":668},{"cell_type":"code","source":"#!g1.1\ndef get_normalization_layer(name, dataset):\n    \n  normalizer = preprocessing.Normalization(axis=None)\n  feature_ds = dataset.map(lambda x, y: x[name])\n  normalizer.adapt(feature_ds)\n\n  return normalizer","metadata":{"cellId":"5blcw8dye27e90var8ukps","trusted":true},"outputs":[],"execution_count":669},{"cell_type":"code","source":"#!g1.1\ndef get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n  # Create a StringLookup layer which will turn strings into integer indices\n  if dtype == 'string':\n    index = preprocessing.StringLookup(max_tokens=max_tokens)\n  else:\n    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n\n  feature_ds = dataset.map(lambda x, y: x[name])\n  index.adapt(feature_ds)\n  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n\n  return lambda feature: encoder(index(feature))\n","metadata":{"cellId":"3xivyvcbwwmys0o1mt61j9","trusted":true},"outputs":[],"execution_count":670},{"cell_type":"code","source":"#!g1.1\nbatch_size = 10\ntest_ds = df_to_dataset(dataset, shuffle=False, batch_size=batch_size)","metadata":{"cellId":"5orzf49hdkhaeaeg2z4y8v","trusted":true},"outputs":[],"execution_count":671},{"cell_type":"code","source":"#!g1.1\nall_inputs = []\nencoded_features = []\n\nfor header in ['cert_type_id', 'cert_vetform_id', 'cert_status_id',\n       'cert_nature_type_id', 'cert_request_type_id', 'cert_reqsource_type_id',\n       'consignor_be_id', 'consignor_ent_id', 'consignee_be_id',\n       'consignee_ent_id', 'sub_product_id', 'product_id', 'doctor_id',\n       'unit_id', 'base_unit_id', 'consignor_be_region_id',\n       'consignor_ent_region_id', 'consignee_be_region_id',\n       'consignee_ent_region_id', 'transfer_type_id', 'product_type_id',\n       'cert_source_id', 'cert_protected_id', 'repaid_doctor_id',\n       'canceled_doctor_id', 'vetExpertise', 'product_name_id', 'former_id',\n       'protocol_version', 'transit_time_hour', 'weight', 'base_weight']:\n    \n  print(header)\n  numeric_col = tf.keras.Input(shape=(1,), name=header)\n  normalization_layer = get_normalization_layer(header, train_ds)\n  encoded_numeric_col = normalization_layer(numeric_col)\n  all_inputs.append(numeric_col)\n  encoded_features.append(encoded_numeric_col)\n\n\ncategorical_cols = ['unit_guid', 'cert_date', 'cert_insert_date',\n       'consignor_be_sub_region_id', 'consignee_be_sub_region_id',\n       'consignor_ent_sub_region_id', 'consignee_ent_sub_region_id',\n       'repaid_cert_date', 'guidOriginCountry',\n       'guidRecipientCountry']\nfor header in categorical_cols:\n  print(header)\n  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n  encoded_categorical_col = encoding_layer(categorical_col)\n  all_inputs.append(categorical_col)\n  encoded_features.append(encoded_categorical_col)","metadata":{"cellId":"c7sa8nlrwy652qhzs852s","trusted":true},"outputs":[],"execution_count":672},{"cell_type":"code","source":"#!g1.1\n#!g1.1\nall_features = tf.keras.layers.concatenate(encoded_features)\nx = tf.keras.layers.Dense(128, activation=\"relu\")(all_features)\nx = tf.keras.layers.Dropout(0.5)(x)\nx = tf.keras.layers.Dense(64, activation=\"relu\")(x)\nx = tf.keras.layers.Dropout(0.5)(x)\nx = tf.keras.layers.Dense(32, activation=\"relu\")(x)\nx = tf.keras.layers.Dropout(0.5)(x)\noutput = tf.keras.layers.Dense(9)(x)\nmodel = tf.keras.Model(all_inputs, output)\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])","metadata":{"cellId":"epnivv8mvr6u0cqw7dbb0q","trusted":true},"outputs":[],"execution_count":674},{"cell_type":"code","source":"\nmodel.load_weights('Weights/weights')\n\nloss, accuracy = model.evaluate(dataset)\nprint(\"Accuracy\", accuracy)","metadata":{"scrolled":true,"cellId":"rcqbzl5zwdiabyhnbdgwy4","trusted":true},"outputs":[],"execution_count":675},{"cell_type":"code","source":"df2 = df.head(90) + df.sort_values(by=['reasons'], ascending=False, axis=0)\ndf2.write_csv(git.csv)","metadata":{"cellId":"weavpvrs06w1mpgoh7xjn","trusted":true},"outputs":[],"execution_count":null}]}